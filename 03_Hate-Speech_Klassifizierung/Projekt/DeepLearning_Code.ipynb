import tensorflow as tf
import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
import tqdm
import csv
from google.colab import drive 
drive.mount('/content/drive')

SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)
EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors
TEST_SIZE = 0.1 # ratio of evaluation set
BATCH_SIZE = 128 # size of one batch 
EPOCHS = 20 # number of epochs


https://github.com/t-davidson/hate-speech-and-offensive-language
#902 rows × 2 columns 
train_data1 = pd.read_csv("hatespeech_task_train.csv",
                            sep='\t',
                            names=["post", "label"])
                            
#https://www.kaggle.com/pandeyakshive97/hate-speech-dataset
#9925 rows × 2 columns
#train_data2 = pd.read_csv("hate_speech.csv",
                            sep=',', 
                            header = 0,                 
                            names=["post", "label"]) 
                            
#https://www.kaggle.com/vkrahul/twitter-hate-speech?select=train_E6oV3lV.csv
#31962 rows × 2 columns
#train_data3 = pd.read_csv("train_E6oV3lV.csv",
                             sep=',',
                             header = 0,
                             names=["label","post"])
                             
#https://www.kaggle.com/mrinaal007/hate-speech-detection?select=toxic_train.csv
#159571 rows × 2 columns 
#train_data4 = pd.read_csv("toxic_train.csv",
                            sep=',',
                            header = 0,
                            names=["post", "label"])
                            
#https://www.kaggle.com/mrinaal007/hate-speech-detection?select=toxic_test.csv
#63979 rows x 2 columns
#train_data5 = pd.read_csv("toxic_test.csv",
                            sep=',',
                            header = 0,
                            names=["post", "label"])
                            
#Test data. https://github.com/t-davidson/hate-speech-and-offensive-language    
#rows 100 x 1 columns
test_data = pd.read_csv("hatespeech_task_test.csv",
                            sep='\t',
                            names=["post", "label"])

#Bad wordlist 
bad_word_list = csv.reader(open("bad_word_list.csv", encoding="utf-8"), delimiter=";")
bad_word_list = [line for line in bad_word_list]
for element in bad_word_list:
    for count in range(1, 7):
        element.pop(1)



#Add CSV_Data togehter using Pandas 
first = pd.DataFrame(train_data1)
second = pd.DataFrame(train_data2)
third = pd.DataFrame(train_data3)
fourth = pd.DataFrame(train_data4)
fifth = pd.DataFrame(train_data5)
train_data1 = first.append(second, ignore_index = True) 
train_data2 = train_data1.append(third, ignore_index = True)
train_data3 = train_data2.append(fourth, ignore_index = True)
train_data_all = train_data3.append(fifth, ignore_index = True)

#rows 266338, dtype: int64
x = train_data_all.pop('post')
y = train_data_all.pop('label')

#rows 100, dtype: int64
xtest = test_data.pop('post')
ytest = test_data.pop('label')



# Text tokenization
# vectorizing text, turning each text into sequence of integers
tokenizer = Tokenizer()
# each entry of the lists to be a token, required before using texts to sequences
tokenizer.fit_on_texts(x)
tokenizer.fit_on_texts(xtest)

# convert to sequence of integers
x = tokenizer.texts_to_sequences(x)
xtest = tokenizer.texts_to_sequences(xxtest)


# convert to numpy arrays
x = np.array(x)
y = np.array(y)
xtest = np.array(xtest)
ytest = np.array(ytest)

# pad sequences at the beginning of each sequence with 0's 
x = pad_sequences(x, maxlen=SEQUENCE_LENGTH)
xtest = pad_sequences(xtest, maxlen=SEQUENCE_LENGTH)

# One hot encode the labels 
y = to_categorical(y)
ytest = to_categorical(ytest)



def get_embedding_vectors(tokenizer, dim=100):
    embedding_index = {}
    with open(f"/content/drive/My Drive/Ki Hausarbeit/glove/glove.twitter.27B.100d.txt", encoding='utf8') as f:
        for line in tqdm.tqdm(f, "Reading GloVe"):
            values = line.split()
            word = values[0]
            vectors = np.asarray(values[1:], dtype='float32')
            embedding_index[word] = vectors

    for line in tqdm.tqdm(bad_word_list, "bad_word_list"):
            word = values[0]
            vectors = np.asarray(values[1:], dtype='float32')
            embedding_index[word] = vectors

    word_index = tokenizer.word_index
    embedding_matrix = np.zeros((len(word_index)+1, dim))
    for word, i in word_index.items():
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            # words not found will be 0s
            embedding_matrix[i] = embedding_vector       
    return embedding_matrix
    
def get_model(tokenizer, lstm_units):
    # get the GloVe embedding vectors
    # twitter embedding Vektors 100d 
    embedding_matrix = get_embedding_vectors(tokenizer)
    model = Sequential()
    model.add(Embedding(len(tokenizer.word_index)+1,
              EMBEDDING_SIZE,
              weights=[embedding_matrix],
              trainable=False,
              input_length=SEQUENCE_LENGTH))
    model.add(Dropout(0.4))
    #?recurrent dropout / dropout layer ??
    model.add(LSTM(lstm_units, return_sequences=True, ))
    model.add(LSTM(lstm_units, return_sequences=True, ))
    model.add(LSTM(lstm_units, return_sequences=True, ))
    model.add(LSTM(lstm_units,))
    model.add(Dropout(0.4))
    model.add(Dense(2, activation="softmax"))
    # compile as rmsprop optimizer
    # metric = accuracy, precision, recall
    model.compile(optimizer="rmsprop", loss="categorical_crossentropy",
                  metrics=["accuracy",tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    model.summary()
    return model
    
x_train, x_eval, y_train, y_eval = train_test_split(x,y, test_size = TEST_SIZE) 
model = get_model(tokenizer=tokenizer, lstm_units=128)
model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)


model.evaluate(x_eval, y_eval)
model.evaluate(xtest, ytest)
model.save("/content/drive/My Drive/Ki Hausarbeit")


#The following code could help to analyse the results 

y_train_pred = model.predict(xtest)
y_train_pred = np.round(y_train_pred)
i = (np.equal(y_train_pred, ytest))
i  = np.nonzero(n == False)

counter = 0
print("Non hate speech but predicted wrong ")
print("-------------------------------------------")
a_array = ((0,1) == y_train_pred)
while counter < len(i):
    if(np.all(a_array[i[counter]])):
      print(xxtest[i[counter]])  
      print(y_train_pred[i[counter]])
    counter += 2

counter = 0
print("Hate speech but preedicted wrong ")
print("-------------------------------------------")
a_array = ((1,0) == y_train_pred)
while counter < len(i):
    if(np.all(a_array[i[counter]])):
      print(xxtest[i[counter]])  
      print(y_train_pred[i[counter]])
    counter += 2

            
